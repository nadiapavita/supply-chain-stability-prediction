# -*- coding: utf-8 -*-
"""SCRM_Preprcessing_part1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gcuq9-G0oXY-KyJKdj5a86e4TR74pQDX
"""

# sumber dataset: https://data.mendeley.com/datasets/gystn6d3r4/1

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
from sklearn.preprocessing import RobustScaler, MinMaxScaler
import pickle

drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/SCRM/SCRM_timeSeries_2018_65L.csv")
data.head()

data.info()

data['Timestamp'] = pd.to_datetime(data['Timestamp'], errors='coerce')
data.info()

data.dropna(subset=['Timestamp'], inplace=True)
df = data.set_index('Timestamp')
df = df.sort_index()
df.info()

df.head()

"""### cleaning data"""

df.duplicated().sum()

df.drop_duplicates(inplace=True)
df.info()

df.isnull().sum()

"""### split & implementation data"""

split_index = int(len(df) * 0.8)

train = df.iloc[:split_index].copy()
test = df.iloc[split_index:]

print(train.shape, test.shape)

train.info()

train[['RI_Distributor1', 'Total_Cost']] = train[['RI_Distributor1', 'Total_Cost']].interpolate(method='time')
train = train.reset_index()
train.info()

import pandas as pd
pd.DataFrame.interpolate?

train.isnull().sum()

cols_dropna = ['RI_Supplier1', 'RI_Manufacturer1', 'RI_Retailer1']
train = train.dropna(subset=cols_dropna)
train.isnull().sum()

"""### EDA"""

train = train.set_index('Timestamp')

train = train.sort_index()
train.info()

train.describe()

ax = train['SCMstability_category'].value_counts().sort_index().plot(kind='bar')
ax.bar_label(ax.containers[0])

sns.heatmap(train.corr(), annot=True, vmin = -1, vmax = 1, cmap = "PiYG")
plt.show()

cols = ['RI_Supplier1', 'RI_Distributor1', 'RI_Manufacturer1','RI_Retailer1', 'Total_Cost']

for col in cols:
    plt.figure(figsize=(8,4))
    sns.boxplot(x=train[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

"""### tangani outlier (capping)"""

train.shape

"""#### ['RI_Supplier1'] quantile"""

Q1 = train['RI_Supplier1'].quantile(0.25)
Q3 = train['RI_Supplier1'].quantile(0.75)
IQR = Q3 - Q1

Q1, IQR, Q3

lower_limit = Q1 - 1.5 * IQR
upper_limit = Q3 + 1.5 * IQR
lower_limit, upper_limit

train[(train['RI_Supplier1']<lower_limit) | (train['RI_Supplier1']>upper_limit)].shape

"""outlier diatas 5%, menggunakan QUANTILE CAPPING"""

train['RI_Supplier1'].quantile([0.01, 0.99])

lower = train['RI_Supplier1'].quantile(0.01)
upper = train['RI_Supplier1'].quantile(0.99)

train['RI_Supplier1'] = train['RI_Supplier1'].clip(lower, upper)

train[(train['RI_Supplier1']<lower) | (train['RI_Supplier1']>upper)].shape

sns.boxplot(x=train['RI_Supplier1'])
plt.show()

"""#### ['RI_Distributor1'] quantile"""

Q1 = train['RI_Distributor1'].quantile(0.25)
Q3 = train['RI_Distributor1'].quantile(0.75)
IQR = Q3 - Q1

Q1, IQR, Q3

lower_limit = Q1 - 1.5 * IQR
upper_limit = Q3 + 1.5 * IQR
lower_limit, upper_limit

train[(train['RI_Distributor1']<lower_limit) | (train['RI_Distributor1']>upper_limit)].shape

"""outlier nya diatas 5%, tidak pake IQR CAPPING"""

train['RI_Distributor1'].quantile([0.01, 0.99])

lower = train['RI_Distributor1'].quantile(0.01)
upper = train['RI_Distributor1'].quantile(0.99)

train['RI_Distributor1'] = train['RI_Distributor1'].clip(lower, upper)

train[(train['RI_Distributor1']<lower) | (train['RI_Distributor1']>upper)].shape

sns.boxplot(x=train['RI_Distributor1'])
plt.show()

"""#### ['RI_Manufacturer1']"""

Q1 = train['RI_Manufacturer1'].quantile(0.25)
Q3 = train['RI_Manufacturer1'].quantile(0.75)
IQR = Q3 - Q1

Q1, IQR, Q3

lower_limit = Q1 - 1.5 * IQR
upper_limit = Q3 + 1.5 * IQR
lower_limit, upper_limit

train[(train['RI_Manufacturer1']<lower_limit) | (train['RI_Manufacturer1']>upper_limit)].shape

"""#### ['RI_Retailer1'] IQR"""

Q1 = train['RI_Retailer1'].quantile(0.25)
Q3 = train['RI_Retailer1'].quantile(0.75)
IQR = Q3 - Q1

Q1, IQR, Q3

lower_limit = Q1 - 1.5 * IQR
upper_limit = Q3 + 1.5 * IQR
lower_limit, upper_limit

train[(train['RI_Retailer1']<lower_limit) | (train['RI_Retailer1']>upper_limit)].shape

"""outlier dibawah 5%, menggunakan IQR CAPPING"""

train['RI_Retailer1'] = train['RI_Retailer1'].clip(lower_limit, upper_limit)

train[(train['RI_Retailer1']<lower_limit) | (train['RI_Retailer1']>upper_limit)].shape

sns.boxplot(x=train['RI_Retailer1'])
plt.show()

"""#### ['Total_Cost'] quantile"""

Q1 = train['Total_Cost'].quantile(0.25)
Q3 = train['Total_Cost'].quantile(0.75)
IQR = Q3 - Q1

Q1, IQR, Q3

lower_limit = max(0.0, Q1 - 1.5 * IQR)
upper_limit = Q3 + 1.5 * IQR
lower_limit, upper_limit

train[(train['Total_Cost']<lower_limit) | (train['Total_Cost']>upper_limit)].shape

"""diatas 5%, quantile capping"""

train['Total_Cost'].quantile([0.01, 0.99])

q_low = train['Total_Cost'].quantile(0.01)
lower = max(q_low, 0)
upper = train['Total_Cost'].quantile(0.99)

train['Total_Cost'] = train['Total_Cost'].clip(lower, upper)

train[(train['Total_Cost']<lower) | (train['Total_Cost']>upper)].shape

train['Total_Cost'].quantile([0.01, 0.99])

sns.boxplot(x=train['Total_Cost'])
plt.show()

"""### Save train_primary dan val_data"""

train.describe()

test.duplicated().sum()

train.to_csv('/content/drive/MyDrive/SCRM/train_capping_part3.csv', index=False)
print("done berhasil 1")
test.to_csv('/content/drive/MyDrive/SCRM/test_clean_part3.csv', index=False)
print("done berhasil 2")

"""### fit scaler LSTM"""

train = pd.read_csv('/content/drive/MyDrive/SCRM/train_capping_part3.csv')
test = pd.read_csv('/content/drive/MyDrive/SCRM/test_clean_part3.csv')

train_LSTM = train.copy()
val_data = test.copy()

X_train = train_LSTM.drop(columns=['SCMstability_category'])
y_train = train_LSTM['SCMstability_category']

X_val = val_data.drop(columns=['SCMstability_category'])
y_val = val_data['SCMstability_category']

X_train.info()
X_val.info()

scalerRobust = RobustScaler()
scalerRobust.fit(X_train)
with open("/content/drive/MyDrive/SCRM/scaler_robust_part3.pkl", 'wb') as f:
    pickle.dump(scalerRobust, f)
print ("done berhasil")

scalerMinMax = MinMaxScaler()
scalerMinMax.fit(X_train)
with open("/content/drive/MyDrive/SCRM/scaler_minmax_part3.pkl", 'wb') as f:
    pickle.dump(scalerMinMax, f)
print ("berhasil done")

from sklearn.preprocessing import RobustScaler
RobustScaler?

with open("/content/drive/MyDrive/SCRM/scaler_robust_part3.pkl", 'rb') as f:
    scalerRobust = pickle.load(f)

with open("/content/drive/MyDrive/SCRM/scaler_minmax_part3.pkl", 'rb') as f:
    minmax = pickle.load(f)

X_train_sc = scalerRobust.transform(X_train)
X_val_sc   = scalerRobust.transform(X_val)

X_train_mm = minmax.transform(X_train)
X_val_mm   = minmax.transform(X_val)

X_train_df = pd.DataFrame(X_train_sc, columns=X_train.columns)
X_val_df   = pd.DataFrame(X_val_sc,   columns=X_val.columns)
y_train_df = pd.DataFrame(y_train.values, columns=['SCMstability_category'])
y_val_df   = pd.DataFrame(y_val.values,   columns=['SCMstability_category'])

train_no_balancing = pd.concat([X_train_df, y_train_df], axis=1)
validasi_data = pd.concat([X_val_df,   y_val_df], axis=1)

train_no_balancing.to_csv('/content/drive/MyDrive/SCRM/train_no_balancing_sc_part3.csv', index=False)
print("done berhasil")
validasi_data.to_csv('/content/drive/MyDrive/SCRM/test_sc_part3.csv', index=False)
print("done berhasil 2")

X_train_df = pd.DataFrame(X_train_mm, columns=X_train.columns)
X_val_df   = pd.DataFrame(X_val_mm,   columns=X_val.columns)
y_train_df = pd.DataFrame(y_train.values, columns=['SCMstability_category'])
y_val_df   = pd.DataFrame(y_val.values,   columns=['SCMstability_category'])

train_no_balancing = pd.concat([X_train_df, y_train_df], axis=1)
validasi_data = pd.concat([X_val_df,   y_val_df], axis=1)

train_no_balancing.to_csv('/content/drive/MyDrive/SCRM/train_no_balancing_mm_part3.csv', index=False)
print("done berhasill")
validasi_data.to_csv('/content/drive/MyDrive/SCRM/test_mm_part3.csv', index=False)
print("done berhasill 2")

train_no_balancing.info()

"""### balancing aug robust

#### reshape dulu
"""

train = pd.read_csv('/content/drive/MyDrive/SCRM/train_no_balancing_sc_part3.csv')
train.info()

X_train = train.drop(columns=['SCMstability_category'])
y_train = train['SCMstability_category']

X_train.info()
y_train.info()

print(X_train.shape)

def make_sequences(X, y, timesteps=10):
    X_seq = []
    y_seq = []
    for i in range(len(X) - timesteps):
        X_seq.append(X[i:i+timesteps])
        y_seq.append(y[i+timesteps-1])
    return np.array(X_seq), np.array(y_seq)

X_train, y_train = make_sequences(X_train, y_train, timesteps=10)

print(X_train.shape)
print(y_train.shape)

"""#### udah 3D"""

!pip install tsaug

import numpy as np
import pandas as pd
from tsaug import AddNoise, Drift
from collections import Counter
import random
import os

SEED = 42
np.random.seed(SEED)
random.seed(SEED) # ini supaya enggak random, jadi kalo di run ulang hasil nya tetep sama

X_seq = X_train
y_seq = y_train
print("Original shape:", X_seq.shape)
print("Original class distribution:", Counter(y_seq))

jitter = AddNoise(
    scale=[0.01] * X_seq.shape[2],
    per_channel=True,
    seed=SEED
)
drift = Drift(max_drift=0.005, seed=SEED)

def augment_timeseries(X):
    X_aug = jitter.augment(X)
    X_aug = drift.augment(X_aug)
    return X_aug

X_balanced = X_seq.copy()
y_balanced = y_seq.copy()

counts = Counter(y_seq)
target = max(counts.values())

for cls, cnt in counts.items():
    if cnt >= target:
        continue

    need = target - cnt
    X_cls = X_seq[y_seq == cls]

    idxs = np.random.choice(len(X_cls), size=need, replace=True)
    X_to_aug = X_cls[idxs]

    X_aug = augment_timeseries(X_to_aug)

    X_balanced = np.concatenate([X_balanced, X_aug], axis=0)
    y_balanced = np.concatenate([y_balanced, np.full(len(X_aug), cls)])

print("Balanced class distribution:", Counter(y_balanced))

# Save na
n, t, f = X_balanced.shape
X_flat = X_balanced.reshape(n, t * f)

df_X = pd.DataFrame(X_flat)
df_y = pd.DataFrame({'SCMstability_category': y_balanced})

df_final = pd.concat([df_X, df_y], axis=1)

save_path = '/content/drive/MyDrive/SCRM/train_balancing_sc.csv'
os.makedirs(os.path.dirname(save_path), exist_ok=True)
df_final.to_csv(save_path, index=False)

print("Saved balanced dataset to:", save_path)
print("Final shape:", df_final.shape)

import matplotlib.pyplot as plt

TIMESTEP_IDX = None        # None = semua timestep
FEATURE_IDX = 0            # ganti kalau mau fitur lain
CLASSES = sorted(np.unique(y_seq))

CLS = CLASSES[4]  # ganti kelas kalau mau

orig_idx = np.where(y_seq == CLS)[0][0]
aug_idx  = np.where(y_balanced == CLS)[-1][0]  # ambil hasil augment

plt.figure(figsize=(14, 5))
plt.plot(X_seq[orig_idx, :, FEATURE_IDX], label='Original', linewidth=2)
plt.plot(X_balanced[aug_idx, :, FEATURE_IDX], label='Augmented', linestyle='--')

plt.title(f'Class {CLS}: Original vs Augmented')
plt.xlabel('Timestep')
plt.ylabel(f'Feature {FEATURE_IDX}')
plt.legend()
plt.grid(True)
plt.show()

"""### balancing minmax

#### reshape dulu
"""

train = pd.read_csv('/content/drive/MyDrive/SCRM/train_no_balancing_mm_part3.csv')

train.info()

X_train = train.drop(columns=['SCMstability_category'])
y_train = train['SCMstability_category']

X_train.info()
y_train.info()

print(X_train.shape)

def make_sequences(X, y, timesteps=10):
    X_seq = []
    y_seq = []
    for i in range(len(X) - timesteps):
        X_seq.append(X[i:i+timesteps])
        y_seq.append(y[i+timesteps-1])
    return np.array(X_seq), np.array(y_seq)

X_train, y_train = make_sequences(X_train, y_train, timesteps=10)

print(X_train.shape)
print(y_train.shape)

!pip install tsaug

import numpy as np
import pandas as pd
from tsaug import AddNoise, Drift
from collections import Counter
import random
import os

# =========================
# Reproducibility
# =========================
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# =========================
# Build LSTM sequences (already done in a previous cell, so just assign existing variables)
# =========================
X_seq = X_train
y_seq = y_train

print("Original shape:", X_seq.shape)
print("Original class distribution:", Counter(y_seq))

# =========================
# Augmenters (SAFE version)
# =========================
jitter = AddNoise(
    scale=[0.01] * X_seq.shape[2],
    per_channel=True,
    seed=SEED
)

drift = Drift(max_drift=0.005, seed=SEED)

def augment_timeseries(X):
    X_aug = jitter.augment(X)
    X_aug = drift.augment(X_aug)
    return X_aug

# =========================
# Balancing via augmentation
# =========================
X_balanced = X_seq.copy()
y_balanced = y_seq.copy()

counts = Counter(y_seq)
target = max(counts.values())

for cls, cnt in counts.items():
    if cnt >= target:
        continue

    need = target - cnt
    X_cls = X_seq[y_seq == cls]

    idxs = np.random.choice(len(X_cls), size=need, replace=True)
    X_to_aug = X_cls[idxs]

    X_aug = augment_timeseries(X_to_aug)

    X_balanced = np.concatenate([X_balanced, X_aug], axis=0)
    y_balanced = np.concatenate([y_balanced, np.full(len(X_aug), cls)])

print("Balanced class distribution:", Counter(y_balanced))

# =========================
# Save (flattened for storage)
# =========================
n, t, f = X_balanced.shape
X_flat = X_balanced.reshape(n, t * f)

df_X = pd.DataFrame(X_flat)
df_y = pd.DataFrame({'SCMstability_category': y_balanced})

df_final = pd.concat([df_X, df_y], axis=1)

save_path = '/content/drive/MyDrive/SCRM/train_balancing_mm.csv'
os.makedirs(os.path.dirname(save_path), exist_ok=True)
df_final.to_csv(save_path, index=False)

print("Saved balanced dataset to:", save_path)
print("Final shape:", df_final.shape)

import matplotlib.pyplot as plt

TIMESTEP_IDX = None        # None = semua timestep
FEATURE_IDX = 0            # ganti kalau mau fitur lain
CLASSES = sorted(np.unique(y_seq))

CLS = CLASSES[4]  # ganti kelas kalau mau

orig_idx = np.where(y_seq == CLS)[0][0]
aug_idx  = np.where(y_balanced == CLS)[-1][0]  # ambil hasil augment

plt.figure(figsize=(14, 5))
plt.plot(X_seq[orig_idx, :, FEATURE_IDX], label='Original', linewidth=2)
plt.plot(X_balanced[aug_idx, :, FEATURE_IDX], label='Augmented', linestyle='--')

plt.title(f'Class {CLS}: Original vs Augmented')
plt.xlabel('Timestep')
plt.ylabel(f'Feature {FEATURE_IDX}')
plt.legend()
plt.grid(True)
plt.show()